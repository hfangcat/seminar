<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on KTH ML Seminars</title>
    <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/</link>
    <description>Recent content in Posts on KTH ML Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Tue, 18 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://heuristic-ardinghelli-1080b7.netlify.app/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mathilde Caron: TBD</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-5/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-5/</guid>
      <description>Title: TBD
Speaker: Mathilde Caron, Facebook AI Research
Date and Time: Tuesday, May 18, 1-2 pm
Place: Zoom Meeting
Abstract: TBD
Bio: TBD</description>
    </item>
    
    <item>
      <title>Kai Han: Transformer in Transformer</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-2/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-2/</guid>
      <description>Title: Transformer in Transformer
Speaker: Kai Han, Huawei Noah’s Ark Lab
Date and Time: Tuesday, April 27, 1-2 pm
Place: Zoom Meeting
Abstract: Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation.</description>
    </item>
    
    <item>
      <title>Alaa El-Nouby: Training Vision Transformers for Image Retrieval</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-4/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-4/</guid>
      <description>Title: Training Vision Transformers for Image Retrieval
Speaker: Alaa El-Nouby, Facebook AI Research and Inria Paris
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Meeting ID: 699 6421 6598 Pass code: 600145
Abstract: Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer.</description>
    </item>
    
    <item>
      <title>Hugo Touvron: Training data-efficient image transformers &amp; distillation through attention / Going deeper with Image Transformers</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-3/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-3/</guid>
      <description>Title: Training data-efficient image transformers &amp;amp; distillation through attention / Going deeper with Image Transformers
Speaker: Hugo Touvron, Facebook AI Research and Sorbonne University
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Meeting ID: 699 6421 6598 Pass code: 600145
Abstract:
Training data-efficient image transformers &amp;amp; distillation through attention
Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.</description>
    </item>
    
    <item>
      <title>Mostafa Dehghani: Scaling up Vision Models with Transformers</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-1/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-1/</guid>
      <description>Title: Scaling up Vision Models with Transformers
Speaker: Mostafa Dehghani, Google Brain
Date and Time: Tuesday, April 13, 1-2 pm
Place: Zoom Meeting
Meeting ID: 698 1002 6609 Pass code: 983918
Abstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
    
    <item>
      <title>Alyosha Efros: The Revolution Will Not Be Supervised</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-6/</guid>
      <description>Title: The Revolution Will Not Be Supervised
Speaker: Alyosha Efros, University of California Berkeley
Date and Time: 2019
Place: Room 304, Teknikringen 14
Abstract: Computer vision has made impressive gains through the use of deep learning models trained with large-scale labeled data. However, labels require expertise and curation and are expensive to collect. Worse, semantic supervision often leads to models that can &amp;ldquo;cheat&amp;rdquo;. Can one discover useful visual representations without the use of explicitly curated labels?</description>
    </item>
    
    <item>
      <title>Daphna Weinshall</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-7/</guid>
      <description>Title:
Speaker: Daphna Weinshall, Hebrew University, Jerusalem
Date and Time: 2019
Place: Room 525, Teknikringen 14
Abstract:
Bio:</description>
    </item>
    
  </channel>
</rss>
