<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on KTH CV/DL Seminars</title>
    <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/</link>
    <description>Recent content in Posts on KTH CV/DL Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Tue, 18 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://heuristic-ardinghelli-1080b7.netlify.app/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mathilde Caron: TBD</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-5/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-5/</guid>
      <description>Title: TBD
Speaker: Mathilde Caron, Facebook AI Research
Date and Time: Tuesday, May 18, 1-2 pm
Place: Zoom Meeting
Abstract: TBD
Bio: TBD</description>
    </item>
    
    <item>
      <title>Kai Han: Transformer in Transformer</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-2/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-2/</guid>
      <description>Title: Transformer in Transformer
Speaker: Kai Han, Huawei Noah’s Ark Lab
Date and Time: Tuesday, April 27, 1-2 pm
Place: Zoom Meeting
Abstract: Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation.</description>
    </item>
    
    <item>
      <title>Alaa El-Nouby: Training Vision Transformers for Image Retrieval</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-4/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-4/</guid>
      <description>Title: Training Vision Transformers for Image Retrieval
Speaker: Alaa El-Nouby, Facebook AI Research and Inria Paris
Date and Time: Tuesday, April 20, 2-3 pm
Place: Zoom Meeting
Abstract: Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer.</description>
    </item>
    
    <item>
      <title>Hugo Touvron: Training data-efficient image transformers &amp; distillation through attention / Going deeper with Image Transformers</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-3/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-3/</guid>
      <description>Title: Training data-efficient image transformers &amp;amp; distillation through attention / Going deeper with Image Transformers
Speaker: Hugo Touvron, Facebook AI Research and Sorbonne University
Date and Time: Tuesday, April 20, 1-2 pm
Place: Zoom Meeting
Abstract:
Training data-efficient image transformers &amp;amp; distillation through attention
Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.</description>
    </item>
    
    <item>
      <title>Mostafa Dehghani: Scaling up Vision Models with Transformers</title>
      <link>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-1/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heuristic-ardinghelli-1080b7.netlify.app/posts/post-1/</guid>
      <description>Title: Scaling up Vision Models with Transformers
Speaker: Mostafa Dehghani, Google Brain
Date and Time: Tuesday, April 13, 1-2 pm
Place: Zoom Meeting
Meeting ID: 698 1002 6609 Pass code: 983918
Abstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
    
  </channel>
</rss>
